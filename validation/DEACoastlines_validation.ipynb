{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEA Coastlines validation\n",
    "\n",
    "To do:\n",
    "* [X] Change output CRS to Australian Albers\n",
    "* [X] Discard validation sides with multiple intersects?\n",
    "* [X] Split analysis code into:\n",
    "    * Aggregate multiple profiles and export into single file\n",
    "    * Analyse and plot single file\n",
    "* [X] Add extraction of environmental data for each profile line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules/functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "lowess = sm.nonparametric.lowess\n",
    "\n",
    "sys.path.append('/g/data/r78/DEACoastlines/')\n",
    "import deacoastlines_validation as deacl_val\n",
    "import deacoastlines_statistics as deacl_stats\n",
    "\n",
    "\n",
    "rename_dict = {\n",
    "    'Beachrock undiff': 'rocky',\n",
    "    'Beachrock undiff dominant': 'rocky',\n",
    "    'Boulder or shingle-grade beach undiff': 'rocky',\n",
    "    'Boulder groyne or breakwater undiff': 'rocky',\n",
    "    'Flat boulder deposit (rock) undiff': 'rocky',\n",
    "    'Hard bedrock shore': 'rocky',\n",
    "    'Hard bedrock shore inferred': 'rocky',\n",
    "    'Hard rock cliff (>5m)': 'rocky',\n",
    "    'Hard rocky shore platform': 'rocky',\n",
    "    'Rocky shore platform (undiff)': 'rocky',\n",
    "    'Sloping boulder deposit (rock) undiff': 'rocky',\n",
    "    'Sloping hard rock shore': 'rocky',\n",
    "    'Sloping soft `bedrock¿ shore': 'rocky',\n",
    "    'Sloping soft \\u2018bedrock\\u2019 shore': 'rocky',\n",
    "    'Soft `bedrock¿ shore inferred': 'rocky',\n",
    "    'Soft `bedrock¿ shore platform': 'rocky',\n",
    "    'Beach (sediment type undiff)': 'sandy',\n",
    "    'Fine-medium sand beach': 'sandy',\n",
    "    'Fine-medium sandy tidal flats': 'sandy',\n",
    "    'Mixed sand and shell beach': 'sandy',\n",
    "    'Mixed sandy shore undiff': 'sandy',\n",
    "    'Perched sandy beach (undiff)': 'sandy',\n",
    "    'Sandy beach undiff': 'sandy',\n",
    "    'Sandy beach with cobbles/pebbles (rock)': 'sandy',\n",
    "    'Sandy shore undiff': 'sandy',\n",
    "    'Sandy tidal flats': 'sandy',\n",
    "    'Sandy tidal flats with coarse stony debris': 'sandy',\n",
    "    'Sandy tidal flats, no bedrock protruding': 'sandy',\n",
    "    'Sloping coffee rock deposit': 'rocky',\n",
    "    'Muddy tidal flats': 'muddy',\n",
    "    'Tidal flats (sediment undiff)': 'muddy',\n",
    "    'Artificial shoreline undiff': 'rocky',\n",
    "    'Artificial boulder structures undiff': 'rocky',\n",
    "    'Boulder revetment': 'rocky',\n",
    "    'Boulder seawall': 'rocky',\n",
    "    'Concrete sea wall': 'rocky',\n",
    "    'Piles (Jetty)': 'rocky',\n",
    "    'Coarse sand beach': 'sandy'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_vector(output_stats, fname='test6.shp', x='0_x', y='0_y', crs='EPSG:3577')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "The following cells standardise varied validation coastal monitoring datasets into a consistent format. \n",
    "This allows the subsequent validation analysis to be applied to all validation data without requiring custom code for each.\n",
    "\n",
    "The standardised output format includes the following columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `id`: A unique string constructed from the beach name, section name and profile name\n",
    "* `date`: A string providing the date of the coastal monitoring survey in format `YYYY-MM-DD`\n",
    "* `beach`: A string giving the name of the beach or site\n",
    "* `section`: A string giving section of a beach or site name if available\n",
    "* `profile`: A string giving the name of the individual coastal monitoring profile\n",
    "* `name`: A string giving a unique name for each overall dataset, e.g. `cgc` for City of Gold Coast\n",
    "* `source`: A standardised string giving the coastal monitoring method used to obtain the data (valid options include `emery/levelling`, `gps`, `aerial photogrammetry`, `total station`, `terrestrial laser scanning`, `satellite`, `lidar`)\n",
    "* `foredune_dist`: An optional floating point number giving the distance along the profile to the top of the foredune. This is used to exclude spurious coastline detections behind the shoreline, e.g. coastal lagoons). This only applies to datasets that contain elevation measurements.\n",
    "* `slope`: An optional float giving the slope of the intertidal zone at the time of the coastal monitoring survey, calculated by running a slope regression on coastline measurements obtained within a buffer distance of the selected tide datum). This only applies to datasets that contain elevation measurements.\n",
    "* `start_x`, `start_y`: Australian Albers (EPSG:3577) x and y coordinates defining the starting point of the coastal monitoring survey\n",
    "* `end_x`, `end_y`: Australian Albers (EPSG:3577) x and y coordinates defining the end point of the coastal monitoring survey\n",
    "* `0_dist`: The along-profile chainage (e.g. distance in metres along the profile from the start coordinates) of the selected tide datum shoreline. This is used to compare against the distance to the intersection with satellite-derived shorelines.\n",
    "* `0_x`, `0_y`: Australian Albers (EPSG:3577) x and y coordinates defining the location of the selected tide datum shoreline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunshine Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['8.Pumicestone - Bribie', '1.Coolum-Sunshine', '5.Dicky Beach', \n",
    "        '7.Kings Beach', '3.Mooloolaba', '2.Mudjimba-Yaroomba', '6.Shelly Beach',\n",
    "        '4.South Mooloolaba']\n",
    "\n",
    "for site in sites:\n",
    "    deacl_val.preprocess_sunshinecoast(site, datum=0, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moruya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_moruya(fname_out='output_data/moruya.csv', datum=0, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victoria/Deakin\n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_vicdeakin(fname='input_data/vicdeakin/z_data_10cm_VIC.csv',\n",
    "                               datum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRL Narrabeen \n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_narrabeen(fname='input_data/wrl/Narrabeen_Profiles_2019.csv',\n",
    "                               datum=0,\n",
    "                               overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSW Beach Profile Database\n",
    "* [X] Renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for fname in glob.glob('input_data/nswbpd/*.csv'):\n",
    "        pool.apply_async(deacl_val.preprocess_nswbpd, \n",
    "                         [fname, 0, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "# fname = '/g/data/r78/DEACoastlines/validation/input_data/nswbpd/photogrammetry_Xsections_Lennox Head.csv'\n",
    "# profiles_df, intercept_df = deacl_val.preprocess_nswbpd(fname, 0, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City of Gold Coast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['BILINGA', 'BROADBEACH', 'BURLEIGH HEADS', 'COOLANGATTA', 'CURRUMBIN',\n",
    "         'DURANABH', 'FINGAL', 'GREENMOUNT HILL', 'KINGSCLIFF', 'KIRRA',\n",
    "         'MAIN BEACH', 'MERMAID BEACH', 'MIAMI', 'Main Beach Cross Sections',\n",
    "         'NARROWNECK', 'NO*TlH KIRRA', 'PALM BEACH', 'POINT DANGER', \n",
    "         'RAINBOW BAY', 'SEAWAY CENTRE LINE', 'SNAPPER ROCKS', \n",
    "         'SOUTH STRADBROKE', 'SURFERS PARADISE', 'THE SPIT', 'TUGUN', \n",
    "         'TWEED RIVER ENTRANCE']\n",
    "# sites=['MAIN BEACH']\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for site in sites:\n",
    "        pool.apply_async(deacl_val.preprocess_cgc, \n",
    "                         [site, 0, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASMARC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sites to iterate over\n",
    "sites = [i.split('/')[2] for i in glob.glob('input_data/tasmarc/*/')]\n",
    "# sites = sites[2:]\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for site in sites:\n",
    "        pool.apply_async(deacl_val.preprocess_tasmarc, \n",
    "                         [site, 0, False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WA DoT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_gdf = gpd.read_file('input_data/WA_tertiaryCC.shp').to_crs('EPSG:3577').iloc[::-1]\n",
    "regions_gdf.index = (regions_gdf.LABEL\n",
    "                     .str.replace(' - ', '_')\n",
    "                     .str.replace('-', '')\n",
    "                     .str.replace(' ', '')\n",
    "                     .str.replace('/', '')\n",
    "                     .str.replace(',', '')\n",
    "                     .str.replace('_', '-')\n",
    "                     .str.lower())\n",
    "regions_gdf.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.coastal_transects_parallel(\n",
    "    regions_gdf,\n",
    "    interval=200,\n",
    "    transect_length=500,\n",
    "    simplify_length=200,\n",
    "    transect_buffer=50,\n",
    "    overwrite=False,\n",
    "    output_path='input_data/coastal_transects_wadot.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(mp.cpu_count()-1) as pool:\n",
    "    for i, _ in regions_gdf.iterrows():\n",
    "        pool.apply_async(deacl_val.preprocess_wadot, \n",
    "                         [regions_gdf.loc[[i]], False])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DaSilva 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_dasilva2021()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WA DoT - Stirling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deacl_val.preprocess_stirling(fname_out='output_data/stirling_stirling.csv',\n",
    "                              datum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SA Department of Environment and Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['200048',\n",
    " '320010',\n",
    " '320011',\n",
    " '330005',\n",
    " '330014',\n",
    " '425001',\n",
    " '425002',\n",
    " '440004',\n",
    " '525019',\n",
    " '525022',\n",
    " '525023',\n",
    " '530009',\n",
    " '545001',\n",
    " '555007',\n",
    " '555012',\n",
    " '815013']\n",
    "\n",
    "fname = f'input_data/sadew/{sites[15]}.CSV'\n",
    "print(fname)\n",
    "profile_df = deacl_val.preprocess_sadew(fname, datum=0, overwrite=True)\n",
    "profile_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# for fname in fname_list:\n",
    "#     preprocess_sadew(fname, datum=0, overwrite=False)\n",
    "\n",
    "fname_list = glob.glob('input_data/sadew/*.CSV')    \n",
    "\n",
    "with mp.Pool(mp.cpu_count()-1) as pool:\n",
    "    for fname in fname_list:\n",
    "        pool.apply_async(deacl_val.preprocess_sadew, \n",
    "                         [fname, 0, True])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fellowes et al. 20221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import re\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "from pyproj import Transformer\n",
    "from itertools import takewhile\n",
    "from scipy import stats\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from shapely.geometry import box, Point, LineString\n",
    "pd.read_excel('input_data/fellowes2021/Fellowes_et_al_2021_SUPP_Estuarine_Beach_Shorelines_V2.xlsx', sheet_name=0, header=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.read_excel('input_data/fellowes2021/Fellowes_et_al_2021_SUPP_Estuarine_Beach_Shorelines_V2.xlsx',\n",
    "                       sheet_name='Profile Locations',\n",
    "                       header=1,\n",
    "                       names=['estuary', 'beach', 'profile', 'start_y', 'start_x', 'end_y', 'end_x' ]).drop('estuary', axis=1)\n",
    "coords['beach'] = coords.beach.str.replace(\" \", \"\").str.replace(\"(\", \"\").str.replace(\")\", \"\").str.replace('Fishermans','Frenchmans').str.lower()\n",
    "coords['section'] = 'all'\n",
    "coords['name'] = 'fellowes2021'\n",
    "coords['source'] = 'aerial photogrammetry'\n",
    "coords['slope'] = np.nan\n",
    "coords['id'] = (coords.beach + '_' + coords.section + '_' + coords.profile)\n",
    "\n",
    "# Reproject coords to Albers and create geodataframe\n",
    "trans = Transformer.from_crs('EPSG:4326', 'EPSG:3577', always_xy=True)\n",
    "coords['start_x'], coords['start_y'] = trans.transform(coords.start_x.values,\n",
    "                                                       coords.start_y.values)\n",
    "coords['end_x'], coords['end_y'] = trans.transform(coords.end_x.values,\n",
    "                                                   coords.end_y.values)\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowes_data = pd.read_excel('input_data/fellowes2021/Fellowes_et_al_2021_SUPP_Estuarine_Beach_Shorelines_V2.xlsx', None)\n",
    "beach_list = list(fellowes_data.keys())[1:]\n",
    "\n",
    "for name in beach_list:\n",
    "\n",
    "    beach = name.split(' - ')[1].replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").lower()\n",
    "    fname_out = f'output_data/fellowes2021_{beach}.csv'\n",
    "    print(f'Processing {beach:<80}', end='\\r')\n",
    "\n",
    "    # Load data and convert to long format\n",
    "    wide_df = fellowes_data[name]\n",
    "    profiles_df = pd.melt(wide_df, \n",
    "                          id_vars='Date', \n",
    "                          var_name='profile', \n",
    "                          value_name='0_dist').rename({'Date': 'date'}, axis=1)\n",
    "    profiles_df['date'] = pd.to_datetime(profiles_df.date, yearfirst=True)\n",
    "    profiles_df['id'] = (f'{beach}_all_' + profiles_df.profile)\n",
    "\n",
    "    # Remove negative distances\n",
    "    profiles_df = profiles_df.loc[profiles_df['0_dist'] >= 0]\n",
    "\n",
    "    # Restrict to post 1987\n",
    "    profiles_df = profiles_df[(profiles_df.date.dt.year > 1987)]\n",
    "\n",
    "    # Merge profile coordinate data into transect data\n",
    "    profiles_df = profiles_df.merge(coords, on=['id', 'profile'])\n",
    "\n",
    "    # Add coordinates at supplied distance along transects\n",
    "    profiles_df[['0_x', '0_y']] = profiles_df.apply(\n",
    "        lambda x: pd.Series(deacl_val.dist_along_transect(x['0_dist'], \n",
    "                                                x.start_x, \n",
    "                                                x.start_y,\n",
    "                                                x.end_x,\n",
    "                                                x.end_y)), axis=1)\n",
    "\n",
    "    # Keep required columns\n",
    "    shoreline_df = profiles_df[['id', 'date', 'beach', \n",
    "                                'section', 'profile', 'name',\n",
    "                                'source', 'slope', 'start_x', \n",
    "                                'start_y', 'end_x', 'end_y', \n",
    "                                '0_dist', '0_x', '0_y']]\n",
    "\n",
    "    shoreline_df.to_csv(fname_out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct DEA Coastlines validation\n",
    "This section compares annual shorelines and rates of change derived from DEA Coastlines with the standardised validation datasets generated in the previous pre-processing step. \n",
    "\n",
    "### Run comparison and load outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "val_paths = glob.glob('output_data/v1.1.0*.csv')\n",
    "random.shuffle(val_paths)\n",
    "deacl_path = '/g/data/r78/DEACoastlines/releases/DEACoastlines_v1.0.0/Shapefile/DEACoastlines_annualcoastlines_v1.0.0.shp'\n",
    "deacl_path = '/g/data/r78/DEACoastlines/releases/DEACoastlines_v1.1.0/Shapefile/DEACoastlines_annualshorelines_v1.1.0.shp'\n",
    "\n",
    "prefix='v1.1.0'\n",
    "\n",
    "# # Parallelised\n",
    "# with mp.Pool(6) as pool:\n",
    "#     for val_path in val_paths:\n",
    "        \n",
    "#         # Run analysis and close resulting figure\n",
    "#         pool.apply_async(deacl_val.deacl_validation, \n",
    "#                          [val_path, deacl_path, 0, prefix, False])\n",
    "\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "# Non-parallel (for testing)\n",
    "for val_path in val_paths:\n",
    "    try:\n",
    "        deacl_val.deacl_validation(val_path, deacl_path, 0, prefix, True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results into a single file\n",
    "print('Combining data')\n",
    "stats_list = glob.glob(f'{prefix}_*.csv')\n",
    "stats_df = pd.concat([pd.read_csv(csv) for csv in stats_list])\n",
    "\n",
    "# Rename smartline categories to smaller subset\n",
    "stats_df['smartline'] = stats_df.smartline.replace(rename_dict)\n",
    "\n",
    "# Export to file\n",
    "# stats_df.to_csv('deacl_results.csv', index=False)\n",
    "\n",
    "# Run stats\n",
    "deacl_val.deacl_val_stats(stats_df.val_dist,\n",
    "                          stats_df.deacl_dist,\n",
    "                          n=stats_df.n,\n",
    "                          remove_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation vector\n",
    "output_name = prefix\n",
    "export_eval(stats_df.set_index('id'), output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual shoreline validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results\n",
    "stats_df = pd.read_csv('deacl_results.csv')\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greater than Landsat frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by substrate, no bias correction\n",
    "by_smartline = stats_df.query(\"n >= 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_smartline_nobias = stats_df.query(\"n >= 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "out = deacl_val.rse_tableformat(by_smartline, by_smartline_nobias, 'smartline')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Less than Landsat frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by substrate, no bias correction\n",
    "by_smartline = stats_df.query(\"n < 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_smartline_nobias = stats_df.query(\"n < 22\").groupby('smartline').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "out = deacl_val.rse_tableformat(by_smartline, by_smartline_nobias, 'smartline')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rates of change validation results\n",
    "\n",
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to sites with long temporal record (at least 10 years of data)\n",
    "long_temporal = stats_df.groupby(['id']).filter(lambda x: len(x) >= 10).set_index('id')\n",
    "\n",
    "# Compute rates of change for both validation and DEAC data\n",
    "deacl_rates = long_temporal.groupby(['id']).apply(lambda x: deacl_stats.change_regress(\n",
    "    y_vals=x.deacl_dist, x_vals=x.year, x_labels=x.year))\n",
    "val_rates = long_temporal.groupby(['id']).apply(lambda x: deacl_stats.change_regress(\n",
    "    y_vals=x.val_dist, x_vals=x.year, x_labels=x.year))\n",
    "\n",
    "# Combine rates of change\n",
    "slope_df = pd.merge(val_rates, \n",
    "                    deacl_rates, \n",
    "                    left_index=True, \n",
    "                    right_index=True, \n",
    "                    suffixes=('_val', '_deacl'))\n",
    "\n",
    "deacl_val.deacl_val_stats(val_dist=slope_df.slope_val, deacl_dist=slope_df.slope_deacl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significant results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df_sig = slope_df.loc[(slope_df.pvalue_deacl <= 0.01) | (slope_df.pvalue_val <= 0.01)]\n",
    "deacl_val.deacl_val_stats(val_dist=slope_df_sig.slope_val, deacl_dist=slope_df_sig.slope_deacl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataset stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export validation sites as shapefile and convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_gdf = gpd.GeoDataFrame(data=stats_df,\n",
    "#                              geometry=gpd.points_from_xy(\n",
    "#                                  x=stats_df.lon,\n",
    "#                                  y=stats_df.lat,\n",
    "#                                  crs='EPSG:4326')).to_crs('EPSG:3577')\n",
    "\n",
    "# stats_gdf.to_file('../bishoptaylor_2020/Validation_extent/validation_points.shp')\n",
    "\n",
    "# # Load and reverse buffer Australian boundary\n",
    "# aus_inside = (gpd.read_file('/g/data/r78/rt1527/shapefiles/australia/australia/cstauscd_r.shp')\n",
    "#  .query(\"FEAT_CODE=='mainland'\")\n",
    "#  .to_crs('EPSG:3577')\n",
    "#  .assign(dissolve=1)\n",
    "#  .dissolve('dissolve')\n",
    "#  .simplify(10000)\n",
    "#  .buffer(-100000)\n",
    "#  .buffer(50000))\n",
    "\n",
    "# # Compute convex hulls for each validation dataset\n",
    "# convex_hulls = stats_gdf.dissolve('name').convex_hull.buffer(50000)\n",
    "\n",
    "# # Clip convex hulls by Australia coastline\n",
    "# gpd.overlay(gpd.GeoDataFrame(geometry=convex_hulls), \n",
    "#             gpd.GeoDataFrame(geometry=aus_inside), \n",
    "#             how='difference').buffer(100000).to_file('../bishoptaylor_2020/Validation_extent/validation_extent.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of validation sites by source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby(\"name\")['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby(\"name\")['year'].agg([np.min,np.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = dict(zip(stats_df.groupby([\"source\"])['year'].count().sort_values().index[0:4], ['Other']*4))\n",
    "rename_dict = {**rename_dict, **{'aerial photogrammetry': 'Aerial photogrammetry',\n",
    "                                 'drone photogrammetry': 'Drone photogrammetry',\n",
    "                                 'hydrographic survey': 'Hydrographic survey',\n",
    "                                 'lidar': 'LiDAR'}}\n",
    "\n",
    "counts_per_year = (stats_df\n",
    "                   .pipe(lambda x: x.assign(source_updated = x.source.replace(rename_dict)))\n",
    "                   .groupby([\"year\", \"source_updated\"])['n']\n",
    "                   .sum()\n",
    "                   .unstack()) \n",
    "counts_per_year = counts_per_year / counts_per_year.sum().sum()\n",
    "counts_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(11, 8))\n",
    "counts_per_year.plot(ax=ax, \n",
    "                     kind='bar', \n",
    "                     stacked=True, \n",
    "                     width=1.0, \n",
    "                     edgecolor='#484746', \n",
    "                     linewidth=1.0, \n",
    "                     color=['#e5c494', '#ff7f0e', '#7295c1', '#8dbe8d', 'lightgrey']\n",
    "                    ) \n",
    "ax.yaxis.set_ticks_position('right')\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.xaxis.label.set_visible(False)\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20], ['0%', '5%', '10%', '15%', '20%']);\n",
    "plt.legend(loc=\"upper left\", ncol=5, bbox_to_anchor=(0, -0.08), fancybox=False, shadow=False, frameon=False)\n",
    "\n",
    "# Export to file\n",
    "fig.savefig(fname='../bishoptaylor_2020/Validation_extent/validation_temporal.png', \n",
    "            dpi=300, pad_inches=0, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby(\"name\")[\"source\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['n'] / out['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.groupby('name')['smartline'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results by slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "by_yearly_obs = stats_df.groupby('n').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n, False)).drop('n', axis=1).reset_index()\n",
    "by_yearly_obs.plot.scatter(x='n', y='mae', ax=ax2, s=15)\n",
    "\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax1.spines[axis].set_linewidth(1.5)\n",
    "    ax2.spines[axis].set_linewidth(1.5)\n",
    "\n",
    "by_slope = stats_df[['slope', 'error_m']].dropna(axis=0) \n",
    "by_slope['slope'] = by_slope.slope.abs()\n",
    "\n",
    "sns.kdeplot(ax=ax1,\n",
    "            data=by_slope['slope'],\n",
    "            data2=by_slope['error_m'],\n",
    "            cmap='YlOrRd',\n",
    "            legend=True, \n",
    "            cbar=True,\n",
    "            shade=True,\n",
    "            shade_lowest=False,\n",
    "            levels=16,\n",
    "            clip=([0, 0.202], [-20, 55]),\n",
    "            cbar_kws={\"use_gridspec\":False, \n",
    "                      \"location\":\"top\",\n",
    "                      \"shrink\":0.5, \n",
    "                      \"anchor\":(0.92, 0.0),\n",
    "                      'label': 'Density',\n",
    "                      'ticklocation':\"left\",\n",
    "                      \"ticks\": [0, 0.1, 0.2, 0.3, 0.4, 0.5]}\n",
    "           )\n",
    "\n",
    "# Add trendline and restrict extent\n",
    "z = lowess(by_yearly_obs['mae'], by_yearly_obs['n'])\n",
    "ax2.plot(z[:, 0], z[:, 1], '--', color = 'black', linewidth = 1.3, zorder=3);\n",
    "\n",
    "ax1.set(ylim=(-25, 55))\n",
    "# # Set axis limita\n",
    "# ax1.set_xticks(np.arange(0, 0.2, 0.02))\n",
    "ax1.set_ylabel('Errors (m)')\n",
    "ax2.set_ylabel('Mean Absolute Error (MAE)')\n",
    "ax1.set_xlabel('Intertidal slope (tan θ)')\n",
    "ax2.set_xlabel('Survey frequency (observations per year)')\n",
    "ax2.set_ylim(0, 25)\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0)\n",
    "ax2.legend(['Trendline (LOESS)'], frameon=False)\n",
    "ax1.axhline(y=0, linestyle='--', color='black', linewidth = 0.5)\n",
    "\n",
    "\n",
    "ax1.annotate('Landward\\nbias', \n",
    "             xytext=(0.1817, 2), \n",
    "             xy=(0.1817, 13.8), \n",
    "             arrowprops={'arrowstyle': '-|>', \n",
    "                         'facecolor': 'black', \n",
    "                         'mutation_scale': 15}, \n",
    "             ha='center')\n",
    "ax1.annotate('Seaward\\nbias', \n",
    "             xytext=(0.1817, -7.5), \n",
    "             xy=(0.1817, -14), \n",
    "             arrowprops={'arrowstyle': '-|>', \n",
    "                         'facecolor': 'black', \n",
    "                         'mutation_scale': 15}, \n",
    "             ha='center')\n",
    "\n",
    "# plt.savefig(fname=f'../bishoptaylor_2020/SlopeObs/FigureX_Effectofslopeandobs.png', \n",
    "#             bbox_inches='tight',\n",
    "#             transparent=True,\n",
    "#             pad_inches=0.05, dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of validation source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_source = stats_df.groupby('source').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n))\n",
    "by_source_nobias = stats_df.groupby('source').apply(\n",
    "    lambda x: deacl_val.deacl_val_stats(x.val_dist, x.deacl_dist, x.n, True))\n",
    "\n",
    "deacl_val.rse_tableformat(by_source, by_source_nobias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of yearly validation observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n > 1].n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (stats_df[stats_df.n == 1].n.sum() / stats_df.n.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n >= 22].n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.n >= 22].n.sum()  / stats_df.n.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap and xy scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export evaluation vector\n",
    "output_name = 'test'\n",
    "# deacl_val.export_eval(stats_df, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats and plot scatterplot\n",
    "stats_subset = stats_df  \n",
    "\n",
    "\n",
    "def val_plot(df, \n",
    "             title='Validation',\n",
    "             scatter=True, \n",
    "             density=True,\n",
    "             time=True, \n",
    "             time_stat='mean',\n",
    "             time_legend_pos=[0.8, 0.035],\n",
    "             offset=0,\n",
    "             extent=(0, 120),\n",
    "             scatter_kwargs={}, \n",
    "             time_kwargs={}):\n",
    "    \n",
    "    # Copy data and apply offset\n",
    "    df = df.copy()\n",
    "    df['error_m'] += offset\n",
    "    df['deacl_dist'] += offset\n",
    "\n",
    "    # Compute stats  \n",
    "    n, mae, rmse, stdev, corr, bias = deacl_val.deacl_val_stats(\n",
    "        val_dist=df.val_dist, \n",
    "        deacl_dist=df.deacl_dist)    \n",
    "    offset_str = 'landward offset' if bias > 0 else 'ocean-ward offset'\n",
    "\n",
    "    if scatter:\n",
    "        \n",
    "        # Plot data as scatterplot\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        df.plot.scatter(x='val_dist',\n",
    "                        y='deacl_dist',\n",
    "                        s=15,\n",
    "                        edgecolors='black',\n",
    "                        linewidth=0,\n",
    "#                         xlim=extent,\n",
    "#                         ylim=extent,\n",
    "                        ax=ax,\n",
    "                        **scatter_kwargs)\n",
    "        \n",
    "        # Add dashed line\n",
    "        \n",
    "        \n",
    "        ax.plot(\n",
    "                np.linspace(df.loc[:, ['deacl_dist', 'val_dist']].values.min(), \n",
    "                            df.loc[:, ['deacl_dist', 'val_dist']].values.max()),\n",
    "                np.linspace(df.loc[:, ['deacl_dist', 'val_dist']].values.min(), \n",
    "                            df.loc[:, ['deacl_dist', 'val_dist']].values.max()),\n",
    "                color='black',\n",
    "                linestyle='dashed')\n",
    "        \n",
    "        ax.set_xlabel(f'{title} (metres along profile)')\n",
    "        ax.set_ylabel(f'DEA Coastlines (metres along profile)')\n",
    "        \n",
    "        # Add annotation\n",
    "        ax.annotate(f'Mean Absolute Error: {mae:.1f} m\\n' \\\n",
    "                    f'RMSE: {rmse:.1f} m\\n' \\\n",
    "                    f'Standard deviation: {stdev:.1f} m\\n' \\\n",
    "                    f'Bias: {bias:.1f} m {offset_str}\\n' \\\n",
    "                    f'Correlation: {corr:.2f}\\n',\n",
    "                    xy=(0.04, 0.75),\n",
    "                    fontsize=14,\n",
    "                    xycoords='axes fraction')\n",
    "\n",
    "        # Set title\n",
    "        plt.gca().set_title(f'DEA Coastlines vs {title}', weight='bold')\n",
    "        \n",
    "        # Export to file        \n",
    "        fig.savefig(f\"{title}_scatter_offset{offset:.1f}.png\", dpi=300)\n",
    "        \n",
    "    if density:\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "        g = sns.kdeplot(ax=ax,\n",
    "                        data=df.val_dist,\n",
    "                        data2=df.deacl_dist, \n",
    "                        cmap='YlOrRd', \n",
    "                        shade=True,\n",
    "                        bw=3,\n",
    "                        shade_lowest=False,\n",
    "                        clip=(extent, extent))\n",
    "\n",
    "        g.set(xlabel=f'{title} (metres along profile)', \n",
    "              ylabel=f'DEA Coastlines (metres along profile)')\n",
    "        ax.set_title(f'DEA Coastlines vs {title}', weight='bold')\n",
    "\n",
    "        # Add stats annotation\n",
    "        ax.annotate(f'Mean Absolute Error: {mae:.1f} m\\n' \\\n",
    "                    f'RMSE: {rmse:.1f} m\\n' \\\n",
    "                    f'Standard deviation: {stdev:.1f} m\\n' \\\n",
    "                    f'Bias: {bias:.1f} m {offset_str}\\n' \\\n",
    "                    f'Correlation: {corr:.2f}\\n',\n",
    "                    xy=(0.04, 0.75),\n",
    "                    fontsize=14,\n",
    "                    xycoords='axes fraction')\n",
    "\n",
    "        # Add diagonal line\n",
    "        plt.gca().plot(np.linspace(*extent), \n",
    "                       np.linspace(*extent),\n",
    "                       color='black',\n",
    "                       linestyle='dashed')\n",
    "        \n",
    "        plt.gca().set_ylim(bottom=extent[0])\n",
    "        plt.gca().set_xlim(left=extent[0])\n",
    "\n",
    "        # Export to file\n",
    "        fig = g.get_figure()\n",
    "        fig.savefig(f\"{title}_heatmap_offset{offset:.1f}.png\", dpi=300)\n",
    "        \n",
    "    if time:\n",
    "        \n",
    "        # Group by beach and apply statistic\n",
    "        stats_grouped = (df.groupby(['beach', 'year'], as_index=False)\n",
    "                         .aggregate(time_stat)\n",
    "                         .rename({'beach': 'id',\n",
    "                                  'deacl_dist': 'DEA Coastlines',\n",
    "                                  'val_dist': title}, axis=1)\n",
    "                         .groupby('id')\n",
    "                         .filter(lambda x: len(x) > 1))\n",
    "\n",
    "        # Melt data into long format for faceted plotting\n",
    "        stats_melted = pd.melt(stats_grouped, \n",
    "                               id_vars=['id', 'year'],\n",
    "                               value_vars=['DEA Coastlines', title],\n",
    "                               value_name='Distance (m)')\n",
    "\n",
    "        # Plot facet data\n",
    "        g = sns.relplot(data=stats_melted,\n",
    "                        x=\"year\", \n",
    "                        y=\"Distance (m)\", \n",
    "                        col=\"id\", \n",
    "                        hue=\"variable\",\n",
    "                        height=1.7, \n",
    "                        aspect=1.0, \n",
    "                        kind=\"line\", \n",
    "                        legend='full', \n",
    "                        col_wrap=5,\n",
    "                        **time_kwargs)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.fig.suptitle(f'DEA Coastlines vs {title}', \n",
    "                       weight='bold', \n",
    "                       ha='right')\n",
    "        \n",
    "        # Simplify titles\n",
    "        g.set_titles(row_template='{row_name}', \n",
    "                     col_template='{col_name}')\n",
    "\n",
    "        # Customise legend\n",
    "        g._legend.texts[0].set_text(\"\")\n",
    "        g._legend.set_bbox_to_anchor(time_legend_pos)\n",
    "        \n",
    "        # Export to file\n",
    "        g.savefig(f\"{title}_time_offset{offset:.1f}.png\", dpi=300)\n",
    "        \n",
    "    return pd.Series({'Mean Absolute Error': mae, \n",
    "                    f'RMSE': rmse,\n",
    "                    f'Standard deviation': stdev,\n",
    "                    f'Bias': f'{bias:.1f} m {offset_str}',\n",
    "                    f'Correlation': corr})\n",
    "\n",
    "# for i, sub in stats_subset.groupby('smartline'):\n",
    "\n",
    "#     # Run analysis\n",
    "#     g = val_plot(df=sub,  # stats_subset,\n",
    "#                  title=i.replace('/', '-'),\n",
    "#                  scatter=True, \n",
    "#                  density=False,\n",
    "#                  time=False,\n",
    "#                  time_stat='median',\n",
    "#                  time_legend_pos=[0.67, 0.11],\n",
    "#                  offset=0,\n",
    "#                  extent=(0, 1000))\n",
    "\n",
    "# Run analysis\n",
    "g = val_plot(df=stats_subset,  # stats_subset,\n",
    "         title='nswbpd_eurobodallabeachessouth',\n",
    "         scatter=True, \n",
    "         density=True,\n",
    "         time=False,\n",
    "         time_stat='median',\n",
    "         time_legend_pos=[0.67, 0.11],\n",
    "         offset=0,\n",
    "         extent=(0, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data along profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from scipy import stats\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datacube.utils.geometry import CRS\n",
    "from shapely.geometry import box, shape\n",
    "\n",
    "# Widgets and WMS\n",
    "from odc.ui import ui_poll, select_on_a_map\n",
    "from ipyleaflet import (Map, WMSLayer, WidgetControl, FullScreenControl, \n",
    "                        DrawControl, basemaps, basemap_to_tiles, TileLayer)\n",
    "from ipywidgets.widgets import Layout, Button, HTML\n",
    "from IPython.display import display\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "\n",
    "def extract_geometry(profile,\n",
    "                     start,\n",
    "                     transect_mode='distance'): \n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Convert geometry to a GeoSeries\n",
    "        profile = gpd.GeoSeries(profile, \n",
    "                                crs='EPSG:4326')\n",
    "        start = gpd.GeoSeries(start, \n",
    "                              crs='EPSG:4326').to_crs('EPSG:3577')\n",
    "\n",
    "        # Load data from WFS\n",
    "        xmin, ymin, xmax, ymax = profile.total_bounds\n",
    "        deacl_wfs = f'https://geoserver.dea.ga.gov.au/geoserver/wfs?' \\\n",
    "                    f'service=WFS&version=1.1.0&request=GetFeature' \\\n",
    "                    f'&typeName=dea:coastlines&maxFeatures=1000' \\\n",
    "                    f'&bbox={ymin},{xmin},{ymax},{xmax},' \\\n",
    "                    f'urn:ogc:def:crs:EPSG:4326'\n",
    "        deacl = gpd.read_file(deacl_wfs)\n",
    "        deacl.crs = 'EPSG:3577'\n",
    "\n",
    "        # Raise exception if no coastlines are returned\n",
    "        if len(deacl.index) == 0:\n",
    "            raise ValueError('No annual coastlines were returned for the '\n",
    "                             'supplied transect. Please select another area.')\n",
    "\n",
    "        # Dissolve by year to remove duplicates, then sort by date\n",
    "        deacl = deacl.dissolve(by='year', as_index=False)\n",
    "        deacl['year'] = deacl.year.astype(int)\n",
    "        deacl = deacl.sort_values('year')\n",
    "\n",
    "        # Extract intersections and determine type\n",
    "        profile = profile.to_crs('EPSG:3577')\n",
    "        intersects = deacl.apply(\n",
    "            lambda x: profile.intersection(x.geometry), axis=1)\n",
    "        intersects = gpd.GeoSeries(intersects[0]) \n",
    "\n",
    "        # Select geometry depending on mode\n",
    "        intersects_type = (intersects.type == 'Point' if \n",
    "                           transect_mode == 'distance' else \n",
    "                           intersects.type == 'MultiPoint')\n",
    "\n",
    "        # Remove annual data according to intersections\n",
    "        deacl_filtered = deacl.loc[intersects_type]\n",
    "        drop_years = ', '.join(deacl.year\n",
    "                               .loc[~intersects_type]\n",
    "                               .astype(str)\n",
    "                               .values.tolist())\n",
    "\n",
    "        # In 'distance' mode, analyse years with one intersection only\n",
    "        if transect_mode == 'distance':  \n",
    "\n",
    "            if drop_years:\n",
    "                print(f'Dropping years due to multiple intersections: {drop_years}\\n')            \n",
    "\n",
    "            # Add start and end coordinate\n",
    "            deacl_filtered['start'] = start.iloc[0]\n",
    "            deacl_filtered['end'] = intersects.loc[intersects_type]\n",
    "\n",
    "        # If any data was returned:\n",
    "        if len(deacl_filtered.index) > 0:   \n",
    "\n",
    "            # Compute distance\n",
    "            deacl_filtered['dist'] = deacl_filtered.apply(\n",
    "                lambda x: x.start.distance(x.end), axis=1)\n",
    "\n",
    "            # Extract values\n",
    "            transect_df = pd.DataFrame(deacl_filtered[['year', 'dist']])\n",
    "            transect_df['dist'] = transect_df.dist.round(2)\n",
    "\n",
    "            # Plot data\n",
    "#             fig, ax = plt.subplots(1, 1, figsize=(5, 8))\n",
    "#             transect_df.plot(x='dist', y='year', ax=ax, label='DEA Coastlines')\n",
    "#             ax.set_xlabel(f'{transect_mode.title()} (metres)')\n",
    "\n",
    "            return transect_df.set_index('year')\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_list = []\n",
    "\n",
    "for val_path in val_paths:\n",
    "    \n",
    "    val_df = pd.read_csv(val_path).groupby('id').first()\n",
    "\n",
    "    # Convert validation start and end locations to linestrings\n",
    "    from shapely.geometry import box, Point, LineString\n",
    "    val_geometry = val_df.apply(\n",
    "        lambda x: LineString([(x.end_x, x.end_y), (x.start_x, x.start_y)]), axis=1)\n",
    "\n",
    "    # Convert geometries to GeoDataFrame\n",
    "    val_gdf = gpd.GeoDataFrame(data=val_df,\n",
    "                               geometry=val_geometry,\n",
    "                               crs='EPSG:3577').to_crs('EPSG:4326')\n",
    "\n",
    "    # Get start coord\n",
    "    val_gdf['start_point'] = val_gdf.apply(\n",
    "        lambda x: Point(x.geometry.coords[1]), axis=1)\n",
    "\n",
    "    for i in val_gdf.index:\n",
    "        print(i)\n",
    "        profile_df = extract_geometry(val_gdf.loc[i].geometry, val_gdf.loc[i].start_point)\n",
    "        if profile_df is not None:\n",
    "            profile_list.append(profile_df.rename({'dist': i}, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(profile_list, axis=1).to_csv('fellowes_et_al_2021_profiles_deacoastlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 2020 timeseries data\n",
    "narrabeen_2020 = pd.read_csv('input_data/wrl/Narrabeen_Timeseries_2020.csv', \n",
    "                                   header=None, \n",
    "                                   names=['beach', 'profile', 'date', '0_dist', 'type'],\n",
    "                                   parse_dates=['date']).query(\"19880101 < date < 20201231 & type == 'WIDTH'\")\n",
    "\n",
    "# Standardise to validation format\n",
    "narrabeen_2020['profile'] = narrabeen_2020['profile'].str.lower()\n",
    "narrabeen_2020['beach'] = 'narrabeen'\n",
    "narrabeen_2020['section'] = 'all'\n",
    "narrabeen_2020['source'] = 'gps'\n",
    "narrabeen_2020['name'] = 'wrl'\n",
    "narrabeen_2020['id'] = narrabeen_2020['beach'] + '_' + narrabeen_2020['section'] + '_' + narrabeen_2020['profile']\n",
    "\n",
    "# Load Narrabeen profile stard/stops\n",
    "narrabeen_profiles = pd.read_csv('output_data/wrl_narrabeen.csv') \n",
    "narrabeen_profiles = narrabeen_profiles[['profile', 'start_x', 'start_y', 'end_x', 'end_y']].drop_duplicates()\n",
    "\n",
    "narrabeen_2020 = pd.merge(left=narrabeen_2020, \n",
    "                          right=narrabeen_profiles,\n",
    "                          on='profile')\n",
    "narrabeen_2020['0_x'] = (narrabeen_2020.start_x + narrabeen_2020.end_x) / 2.0\n",
    "narrabeen_2020['0_y'] = (narrabeen_2020.start_y + narrabeen_2020.end_y) / 2.0\n",
    "narrabeen_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrabeen_2020.drop('type', axis=1).to_csv('output_data/2020test_narrabeen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrabeen_2020_plot = pd.read_csv('output_data/2020test_narrabeen.csv', parse_dates=['date'])\n",
    "narrabeen_2020_plot['year'] = narrabeen_2020_plot.date.dt.year + (narrabeen_2020_plot.date.dt.dayofyear -1)/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "profile = 'pf8'\n",
    "narrabeen_2020_plot.query(f\"profile == '{profile}'\")[['year', '0_dist']].rename({'0_dist': 'Narrabeen-Collaroy Beach Survey Program'}, axis=1).plot(x='year', \n",
    "                                                                                                                                           y='Narrabeen-Collaroy Beach Survey Program', \n",
    "                                                                                                                                             alpha=0.8,\n",
    "                                                                                                                                             ax=ax)\n",
    "stats_subset = stats_df.query(f\"profile == '{profile}'\")\n",
    "stats_subset['year'] = stats_subset['year'] + 0.5\n",
    "stats_subset.rename({'deacl_dist': 'DEA Coastlines'}, axis=1).plot(x='year', y='DEA Coastlines', ax=ax, linewidth=3)\n",
    "plt.suptitle(f'Narrabeen-Collaroy Beach Survey Program profile {profile.upper()}')\n",
    "ax.set_ylabel(\"Beach width (m)\")\n",
    "\n",
    "\n",
    "plt.savefig(f\"2020eval_{profile}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** For assistance with any of the Python code or Jupyter Notebooks in this repository, please post a [Github issue](https://github.com/GeoscienceAustralia/DEACoastLines/issues/new). For questions or more information about this product, sign up to the [Open Data Cube Slack](https://join.slack.com/t/opendatacube/shared_invite/zt-d6hu7l35-CGDhSxiSmTwacKNuXWFUkg) and post on the [`#dea-coastlines`](https://app.slack.com/client/T0L4V0TFT/C018X6J9HLY/details/) channel.\n",
    "\n",
    "**Last modified:** November 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
